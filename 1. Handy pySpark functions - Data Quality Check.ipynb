{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import spark_utils as sut\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "import pyspark.sql.functions as F\n",
    "spark = sut.get_spark_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demos the custom PySpark functions discussed in [Speed up Your ML Projects With Spark -- Handy Custom {pySpark} Functions (I)](https://medium.com/towards-artificial-intelligence/speed-up-your-ml-projects-with-spark-09183e054d3a) published on [Towards AI](https://pub.towardsai.net/). \n",
    "\n",
    "The revelant functions were saved in [spark_utils.py](spark_utils.py) and imported into this notebook for demo by `import spark_utils as sut`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo dataframe\n",
    "\n",
    "While Spark is famous for its ability to work with big data, for demo purposes, I have created a small dataset with an obvious duplicate issue. Do you notice that the two ID fields, ID1 and ID2, do not form a primary key? We will use this table to demo and test our custom functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+----------+-------------+\n",
      "|ID1|ID2|  Name|       DOB|         City|\n",
      "+---+---+------+----------+-------------+\n",
      "|101|  A| Alice|2000-01-01|     New York|\n",
      "|101|  A| Alice|2000-01-01|           NY|\n",
      "|102|  B|   Bob|1990-01-01|  Los Angeles|\n",
      "|102|  B|   Bob|1990-01-01|           LA|\n",
      "|103|  E|  Elly|1982-01-01|San Francisco|\n",
      "|104|  J| Jesse|1995-01-01|      Chicago|\n",
      "|105|  B|Binggy|1987-01-01|  Los Angeles|\n",
      "|105|  B| Bingo|1987-01-01|  Los Angeles|\n",
      "+---+---+------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"ID1\", IntegerType(), True),\n",
    "    StructField(\"ID2\", StringType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"DOB\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Sample data with duplicates based on ID1 and ID2\n",
    "data = [\n",
    "    (101, 'A', 'Alice', '2000-01-01', 'New York'),\n",
    "    (102, 'B', 'Bob', '1990-01-01', 'Los Angeles'),\n",
    "    (103, 'E', 'Elly', '1982-01-01', 'San Francisco'),\n",
    "    (104, 'J', 'Jesse', '1995-01-01', 'Chicago'),\n",
    "    (105, 'B', 'Bingo', '1987-01-01', 'Los Angeles'),\n",
    "    (101, 'A', 'Alice', '2000-01-01', 'NY'),\n",
    "    (102, 'B', 'Bob', '1990-01-01', 'LA'),\n",
    "    (105, 'B', 'Binggy', '1987-01-01', 'Los Angeles'),  \n",
    "]\n",
    "\n",
    "# Create DataFrame with schema\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Convert DOB column to DateType\n",
    "df = df.withColumn(\"DOB\", F.to_date(df[\"DOB\"], \"yyyy-MM-dd\"))\n",
    "\n",
    "df = df.orderBy('ID1','ID2')\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom pySpark functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shape\n",
    "\n",
    "I find the shape attribute of pandas dataframes is pretty convenient, therefore created a custom function to get the shape of spark dataframes too. A few things to note:\n",
    "\n",
    "* This custom shape function prints out comma-formated numbers, which can be especially helpful for big datasets.\n",
    "* It can return the shape tuple for further programmatic use when the print_only parameter is set to False.\n",
    "\n",
    "BTW, you might be delighted to learn that all the functions in this article are equipped with 1) Docstring documentation and 2) Type hints. You are welcome üòÅ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a longer dataframe to demo the comma-formatted print out\n",
    "df_ = df.crossJoin(spark.range(1, 1234567)).drop(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 9,876,528\n",
      "Number of columns: 5\n",
      "9876528 5\n"
     ]
    }
   ],
   "source": [
    "num_row, num_col = sut.shape(df_, print_only = False)\n",
    "print(num_row, num_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print schema alphabetically\n",
    "\n",
    "In pySpark, there is a built-in printSchema function. However, when working with very wide tables, I prefer to have the column names sorted alphabetically so I can check for things more effectively. Here is the function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- DOB: date (nullable = true)\n",
      " |-- ID1: integer (nullable = true)\n",
      " |-- ID2: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sut.print_schema_alphabetically(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## verify primary key\n",
    "\n",
    "A common EDA task is to check the primary key(s) and troubleshoot for duplicates. The three functions below are created for this purpose. First, let‚Äôs look at the is_primary_key function. As its name indicates, this function checks if the specified column(s) forms a primary key in the DataFrame. A few things to note\n",
    "\n",
    "* It returns False when the dataframe is empty, or when any of the specified columns are missing from the dataframe.\n",
    "* It checks for missing values in any of the specified columns and excludes relevant rows from the row counts.\n",
    "* Using the verbose parameter, users can specify whether to print out or suppress detailed info during the function run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ No missing values found in columns: ID1, ID2\n",
      "‚ÑπÔ∏è Total row count: 8\n",
      "‚ÑπÔ∏è Unique row count: 5\n",
      "‚ùå The column(s) ID1, ID2 do not form a primary key.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_cols = ['ID1', 'ID2']\n",
    "sut.is_primary_key(df, id_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# surpress the detailed info during function run\n",
    "sut.is_primary_key(df, id_cols, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Column(s) ID3 do not exist in the DataFrame.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_cols_ = ['ID3']\n",
    "sut.is_primary_key(df, id_cols_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ No missing values found in columns: ID1\n",
      "‚ÑπÔ∏è Total row count: 8\n",
      "‚ÑπÔ∏è Unique row count: 5\n",
      "‚ùå The column(s) ID1 do not form a primary key.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accept one single column name as a string\n",
    "sut.is_primary_key(df, 'ID1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find duplicates\n",
    "\n",
    "Consistent with our inspection of the dummy table, the two ID fields do not form a primary key. Of course, duplicates can exist in real data too, below is the function to identify them. üîé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+------+----------+-----------+\n",
      "|count|ID1|ID2|  Name|       DOB|       City|\n",
      "+-----+---+---+------+----------+-----------+\n",
      "|    2|101|  A| Alice|2000-01-01|   New York|\n",
      "|    2|101|  A| Alice|2000-01-01|         NY|\n",
      "|    2|102|  B|   Bob|1990-01-01|Los Angeles|\n",
      "|    2|102|  B|   Bob|1990-01-01|         LA|\n",
      "|    2|105|  B| Bingo|1987-01-01|Los Angeles|\n",
      "|    2|105|  B|Binggy|1987-01-01|Los Angeles|\n",
      "+-----+---+---+------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not sut.is_primary_key(df, id_cols, verbose = False):\n",
    "    dups = sut.find_duplicates(df, id_cols)\n",
    "    dups.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## identify columns responsible for dups\n",
    "\n",
    "From the above table, it is fairly easy to tell which columns are responsible for duplications in our data.\n",
    "\n",
    "* üîé The City column is responsible for the differences in 101-A and 102-B ID combinations. For example, the dup in 101-A is because the City is recorded both as ‚ÄúNew York‚Äù and ‚ÄúNY‚Äù.\n",
    "* üîé The Name column is responsible for the difference in the 105-B ID combination, where the person‚Äôs name is ‚ÄúBingo‚Äù in one record and ‚ÄúBinggy‚Äù in another.\n",
    "Identifying the root cause of the dups is important for troubleshooting. For instance, based on the discovery above, we should consolidate both city and person names in our data.\n",
    "\n",
    "You can imagine that when we have very wide tables and many more dups, identifying and summarizing the root cause using human eyes üëÄ like we did above becomes much trickier.\n",
    "\n",
    "The cols_responsible_for_id_dups function comes in rescue by summarizing the difference_counts for each column based on the primary key(s) provided. üòé For example, in the output below, we can easily see that the field City is responsible for differences in two unique ID combinations, while the Name column is responsible for the dups in one ID pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+\n",
      "|col_name|difference_counts|\n",
      "+--------+-----------------+\n",
      "|    City|                2|\n",
      "|    Name|                1|\n",
      "|     DOB|                0|\n",
      "+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not sut.is_primary_key(df, id_cols, verbose = False):\n",
    "    dup_cols = sut.cols_responsible_for_id_dups(df, id_cols)\n",
    "    dup_cols.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dedupe the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+----------+-------------+\n",
      "|ID1|ID2|  Name|       DOB|         City|\n",
      "+---+---+------+----------+-------------+\n",
      "|101|  A| Alice|2000-01-01|     New York|\n",
      "|102|  B|   Bob|1990-01-01|  Los Angeles|\n",
      "|103|  E|  Elly|1982-01-01|San Francisco|\n",
      "|104|  J| Jesse|1995-01-01|      Chicago|\n",
      "|105|  B|Binggy|1987-01-01|  Los Angeles|\n",
      "+---+---+------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dedup_df = sut.deduplicate_by_rank(df, id_cols, 'City', ascending=False, tiebreaker_col='Name', verbose=False)\n",
    "dedup_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ No missing values found in columns: ID1, ID2\n",
      "‚ÑπÔ∏è Total row count: 5\n",
      "‚ÑπÔ∏è Unique row count: 5\n",
      "üîë The column(s) ID1, ID2 form a primary key.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sut.is_primary_key(dedup_df, id_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## value counts with percent\n",
    "\n",
    "The columns responsible for the most duplicates are listed at the top of the summary table above. We can then analyze these columns further for troubleshooting. If you have a very wide table, narrowing down the investigation like this can be pretty handy. For example, you can zoom in by checking relevant columns‚Äô value counts among the dups. And of course, I have a custom function ready for you to do just this. üòú This function is very much like the value_counts in pandas, with two additional features\n",
    "\n",
    "* percentage for each unique value\n",
    "* comma-formated numbers in the printout\n",
    "\n",
    "Let‚Äôs see it in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-----+\n",
      "|       City|count|  pct|\n",
      "+-----------+-----+-----+\n",
      "|Los Angeles|    3| 50.0|\n",
      "|         LA|    1|16.67|\n",
      "|         NY|    1|16.67|\n",
      "|   New York|    1|16.67|\n",
      "+-----------+-----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[City: string, count: bigint, pct: double]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sut.value_counts_with_pct(dups, 'City')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
