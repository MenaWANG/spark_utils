{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import spark_utils as sut\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "spark = sut.get_spark_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+--------+\n",
      "|  ID|amount|quantity|\n",
      "+----+------+--------+\n",
      "|A001|100.68|       5|\n",
      "|A001|150.33|       3|\n",
      "|A001|799.99|       4|\n",
      "|A002|200.45|       2|\n",
      "|A002|300.89|       4|\n",
      "|A002|899.66|       3|\n",
      "|A003|250.77|       6|\n",
      "|A003|400.13|       3|\n",
      "|A003|999.11|       5|\n",
      "+----+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"ID\", StringType(), False),\n",
    "    StructField(\"amount\", DoubleType(), False),\n",
    "    StructField(\"quantity\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "# Create sample data\n",
    "data = [\n",
    "    (\"A001\", 100.68, 5),    \n",
    "    (\"A001\", 150.33, 3),\n",
    "    (\"A001\", 799.99, 4),\n",
    "    \n",
    "    (\"A002\", 200.45, 2),    \n",
    "    (\"A002\", 300.89, 4),\n",
    "    (\"A002\", 899.66, 3),\n",
    "    \n",
    "    (\"A003\", 250.77, 6),    \n",
    "    (\"A003\", 400.13, 3),\n",
    "    (\"A003\", 999.11, 5)\n",
    "]\n",
    "\n",
    "# Create the Spark DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping and Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_id = df.groupBy(df.ID).agg(\n",
    " F.mean('amount').alias('mean_amount'),\n",
    " F.median('amount').alias('median_amount'), \n",
    " F.min('amount').alias('min_amount'),\n",
    " F.max('amount').alias('max_amount'),\n",
    " F.count('amount').alias('count_amount'),\n",
    " F.count_distinct('amount').alias('count_distinct_amount'),\n",
    " F.sum('amount').alias('sum_amount'),\n",
    " F.sum_distinct('amount').alias('sum_distinct_amount')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+-------------+----------+----------+------------+---------------------+----------+-------------------+\n",
      "|  ID|      mean_amount|median_amount|min_amount|max_amount|count_amount|count_distinct_amount|sum_amount|sum_distinct_amount|\n",
      "+----+-----------------+-------------+----------+----------+------------+---------------------+----------+-------------------+\n",
      "|A003|550.0033333333333|       400.13|    250.77|    999.11|           3|                    3|   1650.01|            1650.01|\n",
      "|A002|            467.0|       300.89|    200.45|    899.66|           3|                    3|    1401.0|             1401.0|\n",
      "|A001|350.3333333333333|       150.33|    100.68|    799.99|           3|                    3|    1051.0|             1051.0|\n",
      "+----+-----------------+-------------+----------+----------+------------+---------------------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_by_id.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-------------+----------+----------+------------+---------------------+----------+-------------------+\n",
      "|  ID|mean_amount|median_amount|min_amount|max_amount|count_amount|count_distinct_amount|sum_amount|sum_distinct_amount|\n",
      "+----+-----------+-------------+----------+----------+------------+---------------------+----------+-------------------+\n",
      "|A003|      550.0|       400.13|    250.77|    999.11|           3|                    3|   1650.01|            1650.01|\n",
      "|A002|      467.0|       300.89|    200.45|    899.66|           3|                    3|    1401.0|             1401.0|\n",
      "|A001|     350.33|       150.33|    100.68|    799.99|           3|                    3|    1051.0|             1051.0|\n",
      "+----+-----------+-------------+----------+----------+------------+---------------------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sut.round_numeric_cols(df_by_id).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Droppinga Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+-------------+----------+----------+----------+\n",
      "|  ID|      mean_amount|median_amount|min_amount|max_amount|sum_amount|\n",
      "+----+-----------------+-------------+----------+----------+----------+\n",
      "|A001|350.3333333333333|       150.33|    100.68|    799.99|    1051.0|\n",
      "|A002|            467.0|       300.89|    200.45|    899.66|    1401.0|\n",
      "|A003|550.0033333333333|       400.13|    250.77|    999.11|   1650.01|\n",
      "+----+-----------------+-------------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols_to_drop = ['count_amount', 'count_distinct_amount', 'sum_distinct_amount']\n",
    "df_by_id.drop(*cols_to_drop).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+--------+----------+\n",
      "|  ID|amount|quantity|start_date|\n",
      "+----+------+--------+----------+\n",
      "|A001|100.68|       5|15/01/2022|\n",
      "|A001|150.33|       3|15/01/2022|\n",
      "|A001|799.99|       4|15/01/2022|\n",
      "|A002|200.45|       2|30/06/2022|\n",
      "|A002|300.89|       4|30/06/2022|\n",
      "|A002|899.66|       3|30/06/2022|\n",
      "|A003|250.77|       6|01/03/2023|\n",
      "|A003|400.13|       3|01/03/2023|\n",
      "|A003|999.11|       5|01/03/2023|\n",
      "+----+------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create dimension table with start dates\n",
    "dim_schema = StructType([\n",
    "    StructField(\"ID\", StringType(), False),\n",
    "    StructField(\"start_date\", StringType(), False)\n",
    "])\n",
    "\n",
    "dim_data = [\n",
    "    (\"A001\", \"15/01/2022\"),\n",
    "    (\"A002\", \"30/06/2022\"),\n",
    "    (\"A003\", \"01/03/2023\")\n",
    "]\n",
    "\n",
    "dim_df = spark.createDataFrame(dim_data, dim_schema)\n",
    "\n",
    "# Simple join of the two tables\n",
    "df_with_dates = df.join(\n",
    "    other=dim_df,\n",
    "    on=\"ID\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Show results\n",
    "df_with_dates.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating New Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_dates = sut.transform_date_cols(\n",
    "    df=df_with_dates,\n",
    "    date_cols=['start_date'],\n",
    "    str_date_format='dd/MM/yyyy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = false)\n",
      " |-- amount: double (nullable = false)\n",
      " |-- quantity: integer (nullable = false)\n",
      " |-- start_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sut.print_schema_alphabetically(df_with_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+--------+----------+------------------+--------------+----------------+------------------+\n",
      "|  ID|amount|quantity|start_date|             total|tenure_in_days|tenure_in_months|   tenure_in_years|\n",
      "+----+------+--------+----------+------------------+--------------+----------------+------------------+\n",
      "|A001|100.68|       5|2022-01-15|503.40000000000003|          1133|     37.19354839| 3.099462365833333|\n",
      "|A001|150.33|       3|2022-01-15|            450.99|          1133|     37.19354839| 3.099462365833333|\n",
      "|A001|799.99|       4|2022-01-15|           3199.96|          1133|     37.19354839| 3.099462365833333|\n",
      "|A002|200.45|       2|2022-06-30|             400.9|           967|     31.70967742| 2.642473118333333|\n",
      "|A002|300.89|       4|2022-06-30|           1203.56|           967|     31.70967742| 2.642473118333333|\n",
      "|A002|899.66|       3|2022-06-30|           2698.98|           967|     31.70967742| 2.642473118333333|\n",
      "|A003|250.77|       6|2023-03-01|1504.6200000000001|           723|     23.64516129|1.9704301075000001|\n",
      "|A003|400.13|       3|2023-03-01|1200.3899999999999|           723|     23.64516129|1.9704301075000001|\n",
      "|A003|999.11|       5|2023-03-01|           4995.55|           723|     23.64516129|1.9704301075000001|\n",
      "+----+------+--------+----------+------------------+--------------+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_dates = df_with_dates.withColumn('total', F.col('amount') * F.col('quantity')) \\\n",
    "       .withColumn('tenure_in_days', F.datediff(F.current_date(), F.col('start_date'))) \\\n",
    "       .withColumn('tenure_in_months', F.months_between(F.current_date(), F.col('start_date'))) \\\n",
    "       .withColumn('tenure_in_years', F.col('tenure_in_months') / 12)\n",
    "\n",
    "df_with_dates.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = false)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- quantity: integer (nullable = false)\n",
      " |-- start_date: date (nullable = true)\n",
      " |-- tenure_in_days: integer (nullable = true)\n",
      " |-- tenure_in_months: double (nullable = true)\n",
      " |-- tenure_in_years: double (nullable = true)\n",
      " |-- total: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sut.print_schema_alphabetically(df_with_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+--------+----------+-------+--------------+----------------+------------------+\n",
      "|  ID|amount|quantity|start_date|  total|tenure_in_days|tenure_in_months|   tenure_in_years|\n",
      "+----+------+--------+----------+-------+--------------+----------------+------------------+\n",
      "|A001|100.68|       5|2022-01-15|  503.4|          1133|           37.19| 3.099462365833333|\n",
      "|A001|150.33|       3|2022-01-15| 450.99|          1133|           37.19| 3.099462365833333|\n",
      "|A001|799.99|       4|2022-01-15|3199.96|          1133|           37.19| 3.099462365833333|\n",
      "|A002|200.45|       2|2022-06-30|  400.9|           967|           31.71| 2.642473118333333|\n",
      "|A002|300.89|       4|2022-06-30|1203.56|           967|           31.71| 2.642473118333333|\n",
      "|A002|899.66|       3|2022-06-30|2698.98|           967|           31.71| 2.642473118333333|\n",
      "|A003|250.77|       6|2023-03-01|1504.62|           723|           23.65|1.9704301075000001|\n",
      "|A003|400.13|       3|2023-03-01|1200.39|           723|           23.65|1.9704301075000001|\n",
      "|A003|999.11|       5|2023-03-01|4995.55|           723|           23.65|1.9704301075000001|\n",
      "+----+------+--------+----------+-------+--------------+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols_to_round = ['total','tenure_in_months']\n",
    "df_with_dates = sut.round_given_cols(df_with_dates, cols_to_round)\n",
    "df_with_dates.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
