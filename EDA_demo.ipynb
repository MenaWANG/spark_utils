{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import spark_utils as sut\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+----------+-------------+\n",
      "|ID1|ID2|  Name|       DOB|         City|\n",
      "+---+---+------+----------+-------------+\n",
      "|101|  A| Alice|2000-01-01|     New York|\n",
      "|101|  A| Alice|2000-01-01|           NY|\n",
      "|102|  B|   Bob|1990-01-01|           LA|\n",
      "|102|  B|   Bob|1990-01-01|  Los Angeles|\n",
      "|103|  E|  Elly|1982-01-01|San Francisco|\n",
      "|104|  J| Jesse|1995-01-01|      Chicago|\n",
      "|105|  B| Bingo|1987-01-01|  Los Angeles|\n",
      "|105|  B|Binggy|1987-01-01|  Los Angeles|\n",
      "+---+---+------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkDemo\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"ID1\", IntegerType(), True),\n",
    "    StructField(\"ID2\", StringType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"DOB\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Sample data with duplicates based on ID1 and ID2\n",
    "data = [\n",
    "    (101, 'A', 'Alice', '2000-01-01', 'New York'),\n",
    "    (102, 'B', 'Bob', '1990-01-01', 'Los Angeles'),\n",
    "    (103, 'E', 'Elly', '1982-01-01', 'San Francisco'),\n",
    "    (104, 'J', 'Jesse', '1995-01-01', 'Chicago'),\n",
    "    (105, 'B', 'Bingo', '1987-01-01', 'Los Angeles'),\n",
    "    (101, 'A', 'Alice', '2000-01-01', 'NY'),\n",
    "    (102, 'B', 'Bob', '1990-01-01', 'LA'),\n",
    "    (105, 'B', 'Binggy', '1987-01-01', 'Los Angeles'),  \n",
    "]\n",
    "\n",
    "# Create DataFrame with schema\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Convert DOB column to DateType\n",
    "df = df.withColumn(\"DOB\", to_date(df[\"DOB\"], \"yyyy-MM-dd\"))\n",
    "\n",
    "df = df.orderBy('ID1','ID2')\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom pySpark functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 9,876,528\n",
      "Number of columns: 5\n"
     ]
    }
   ],
   "source": [
    "df_ = df.crossJoin(spark.range(1, 1234567)).drop(\"id\")\n",
    "sut.shape(df_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## value counts with percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+----+\n",
      "|  Name|    count| pct|\n",
      "+------+---------+----+\n",
      "| Alice|2,469,132|25.0|\n",
      "|   Bob|2,469,132|25.0|\n",
      "|  Elly|1,234,566|12.5|\n",
      "| Jesse|1,234,566|12.5|\n",
      "| Bingo|1,234,566|12.5|\n",
      "|Binggy|1,234,566|12.5|\n",
      "+------+---------+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, count: bigint, pct: double]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sut.value_counts_with_pct(df_, 'Name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print schema alphabetically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- DOB: date (nullable = true)\n",
      " |-- ID1: integer (nullable = true)\n",
      " |-- ID2: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sut.print_schema_alphabetically(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## verify primary key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total row count after filtering out missings: 8\n",
      "Unique row count after filtering out missings: 5\n",
      "The column(s) ID1, ID2 does not form a primary key.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_cols = ['ID1', 'ID2']\n",
    "sut.is_primary_key(df, id_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total row count after filtering out missings: 8\n",
      "Unique row count after filtering out missings: 5\n",
      "The column(s) ID1, ID2 does not form a primary key.\n",
      "+-----+---+---+------+----------+-----------+\n",
      "|count|ID1|ID2|  Name|       DOB|       City|\n",
      "+-----+---+---+------+----------+-----------+\n",
      "|    2|101|  A| Alice|2000-01-01|   New York|\n",
      "|    2|101|  A| Alice|2000-01-01|         NY|\n",
      "|    2|102|  B|   Bob|1990-01-01|Los Angeles|\n",
      "|    2|102|  B|   Bob|1990-01-01|         LA|\n",
      "|    2|105|  B| Bingo|1987-01-01|Los Angeles|\n",
      "|    2|105|  B|Binggy|1987-01-01|Los Angeles|\n",
      "+-----+---+---+------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if sut.is_primary_key(df, id_cols) == False:\n",
    "    dups = sut.find_duplicates(df, id_cols)\n",
    "    dups.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## identify columns responsible for dups\n",
    "\n",
    "With our simple dummy table, it is relatively easy to tell which columns are responsible for the dups. The `City` column is responsible for the differences in 101-A and 102-B ID combinations, while the `Name` column is responsible for the difference in the 105-B combination. \n",
    "\n",
    "Identifying the sources for the dups is important for us to find the suitable fix. But when we have very wide table and many more dups, this task becomes much trickier. \n",
    "\n",
    "The `cols_responsible_for_id_dups` function comes in rescue by summarizing the `difference_counts` for each column based on the primary key(s) provided. For example, in the output below, we can easily see that the field `City` is responsible for differences in two unique ID combinations while `Name` is responsible for one pair. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total row count after filtering out missings: 8\n",
      "Unique row count after filtering out missings: 5\n",
      "The column(s) ID1, ID2 does not form a primary key.\n",
      "+--------+-----------------+\n",
      "|col_name|difference_counts|\n",
      "+--------+-----------------+\n",
      "|    City|                2|\n",
      "|    Name|                1|\n",
      "|     DOB|                0|\n",
      "+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if sut.is_primary_key(df, id_cols) == False:\n",
    "    dup_cols = sut.cols_responsible_for_id_dups(df, id_cols)\n",
    "    dup_cols.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter by string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_strings = ['NY', 'New York']\n",
    "check = sut.filter_df_by_strings(df, 'City', search_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+----------+--------+\n",
      "|ID1|ID2| Name|       DOB|    City|\n",
      "+---+---+-----+----------+--------+\n",
      "|101|  A|Alice|2000-01-01|      NY|\n",
      "|101|  A|Alice|2000-01-01|New York|\n",
      "+---+---+-----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
